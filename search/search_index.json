{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Global Building Inventory (GloBI)","text":"<p>GloBI is a distributed building energy simulation framework designed for regional-scale analysis. It automates the process of transforming geospatial building data into energy models, running simulations in parallel, and collecting results for urban planning and decarbonization strategies.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Automated Pipeline: From GIS data to simulation results with minimal manual intervention</li> <li>Distributed Computing: Scale horizontally using Hatchet workflow orchestration and Docker workers</li> <li>Semantic Mapping: Flexible building typology and component mapping system</li> <li>Regional Scale: Process thousands of buildings in parallel</li> <li>Reproducibility: Version-controlled experiments with full provenance tracking</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<p>To understand how GloBI works, check out our architecture diagrams:</p> <ul> <li>Simple Overview: High-level workflow from inputs to outputs</li> <li>Detailed Design: Comprehensive system architecture with all components</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Follow our tutorials to start using GloBI:</p> <ul> <li>Requirements: Installation and setup</li> <li>Simulation Tasking: Running your first experiment</li> </ul>"},{"location":"#cli-reference","title":"CLI Reference","text":"<p>Explore the CLI documentation for detailed command usage.</p>"},{"location":"architecture-detailed/","title":"GloBI Architecture - Detailed System Design","text":"<p>This diagram provides a comprehensive view of the GloBI system architecture, including all major components, data flows, and external dependencies.</p> <pre><code>%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#4a9eff','primaryTextColor':'#000','primaryBorderColor':'#2563eb','lineColor':'#64748b','secondaryColor':'#fbbf24','tertiaryColor':'#34d399','noteBkgColor':'#fef3c7','noteTextColor':'#000','noteBorderColor':'#f59e0b'}}}%%\nflowchart TD\n    %% User Inputs\n    subgraph INPUTS[\"System Inputs\"]\n        M[Manifest File&lt;br/&gt;GloBIExperimentSpec]\n        GIS[GIS Building Data&lt;br/&gt;Shapefile/GeoJSON/GPKG]\n        CDB[Component Database&lt;br/&gt;SQLite via Prisma]\n        SF[Semantic Fields&lt;br/&gt;YAML]\n        CM[Component Map&lt;br/&gt;YAML]\n        EPW[Weather Files&lt;br/&gt;EPW Archive]\n    end\n\n    %% CLI Layer\n    subgraph CLI[\"CLI Layer (main.py)\"]\n        CLI1[submit manifest]\n        CLI2[simulate]\n        CLI3[get experiment]\n        CLI4[ui]\n        CLI5[output_viz]\n    end\n\n    %% Configuration Layer\n    subgraph CONFIG[\"Configuration Layer (models/configs.py)\"]\n        EXP[GloBIExperimentSpec]\n        FC[FileConfig]\n        GPC[GISPreprocessorConfig]\n        HDC[HourlyDataConfig]\n    end\n\n    %% GIS Processing\n    subgraph GISPROCESS[\"GIS Preprocessing (pipelines.preprocess_gis_file)\"]\n        direction TB\n        GP1[Load GIS File&lt;br/&gt;GeoPandas]\n        GP2[Validate &amp; Reproject CRS]\n        GP3[Rename Columns&lt;br/&gt;Handle Shapefile Limits]\n        GP4[Validate Semantic Fields]\n        GP5[Generate/Validate IDs]\n        GP6[Extract Coordinates]\n        GP7[Filter by Height/Floors]\n        GP8[Filter by WWR]\n        GP9[Filter by Basement/Attic]\n        GP10[Validate Geometry]\n        GP11[Create Rotated Rectangles&lt;br/&gt;gis/geometry.py]\n        GP12[Filter by Area &amp; Edge Length]\n        GP13[Compute Neighbor Indices]\n        GP14[Extract Neighbor Geometries]\n        GP15[Inject Semantic Context]\n        GP16[Assign Weather Files&lt;br/&gt;gis/weather.py]\n\n        GP1 --&gt; GP2 --&gt; GP3 --&gt; GP4 --&gt; GP5\n        GP5 --&gt; GP6 --&gt; GP7 --&gt; GP8 --&gt; GP9\n        GP9 --&gt; GP10 --&gt; GP11 --&gt; GP12 --&gt; GP13\n        GP13 --&gt; GP14 --&gt; GP15 --&gt; GP16\n    end\n\n    %% Allocation\n    subgraph ALLOC[\"Allocation Layer (allocate.py)\"]\n        direction TB\n        A1[For Each Building Row:&lt;br/&gt;Create GloBIBuildingSpec]\n        A2[Calculate Branching Factor&lt;br/&gt;Based on Payload Size]\n        A3[Create BaseExperiment&lt;br/&gt;with simulate_globi_building]\n        A4[Configure RecursionMap&lt;br/&gt;Distribution Strategy]\n        A5[Submit to Hatchet&lt;br/&gt;experiment.allocate]\n    end\n\n    %% Distributed Computing\n    subgraph DIST[\"Distributed Computing Infrastructure\"]\n        direction TB\n        H[Hatchet Workflow&lt;br/&gt;Orchestrator]\n        W1[Docker Worker 1]\n        W2[Docker Worker 2]\n        WN[Docker Worker N]\n        S3[S3/Cloud Storage]\n\n        H --&gt; W1\n        H --&gt; W2\n        H --&gt; WN\n    end\n\n    %% Simulation\n    subgraph SIM[\"Energy Simulation (pipelines.simulate_globi_building)\"]\n        direction TB\n        S1[Receive GloBIBuildingSpec]\n        S2[Construct Zone Definition&lt;br/&gt;from Semantic Fields]\n        S3[Build EnergyPlus Model&lt;br/&gt;epinterface.sbem]\n        S4[Validate Conditioned Areas]\n        S5[Run EnergyPlus Simulation&lt;br/&gt;model.run]\n        S6[Extract Results from SQL&lt;br/&gt;Monthly Energy &amp; Peak]\n        S7[Extract Hourly Data&lt;br/&gt;Optional Timeseries]\n        S8[Create GloBIOutputSpec&lt;br/&gt;DataFrames + Metadata]\n\n        S1 --&gt; S2 --&gt; S3 --&gt; S4 --&gt; S5\n        S5 --&gt; S6 --&gt; S7 --&gt; S8\n    end\n\n    %% Results Aggregation\n    subgraph AGG[\"Results Aggregation (Scythe Framework)\"]\n        direction TB\n        R1[Collect Outputs from Workers]\n        R2[Aggregate DataFrames&lt;br/&gt;Results + HourlyData]\n        R3[Apply Semantic Versioning]\n        R4[Store in Cloud&lt;br/&gt;Parquet Format]\n    end\n\n    %% Output\n    subgraph OUTPUT[\"Output Layer\"]\n        direction TB\n        O1[Download from S3&lt;br/&gt;to Local Directory]\n        O2[Results.parquet&lt;br/&gt;Monthly Energy Data]\n        O3[HourlyData.parquet&lt;br/&gt;Timeseries Optional]\n        O4[Generate Visualization&lt;br/&gt;D3 Dashboard]\n        O5[CSV Exports]\n    end\n\n    %% External Dependencies\n    subgraph EXT[\"External Dependencies\"]\n        EPL[EnergyPlus&lt;br/&gt;Simulation Engine]\n        EPI[EPInterface&lt;br/&gt;IDF Generation]\n        ARC[Archetypal&lt;br/&gt;Building Templates]\n        SCY[Scythe&lt;br/&gt;Distributed Framework]\n        PRIS[Prisma&lt;br/&gt;Database ORM]\n    end\n\n    %% Data Flow Connections\n    M --&gt; CLI1\n    GIS --&gt; CLI1\n    CDB --&gt; CLI1\n    SF --&gt; CLI1\n    CM --&gt; CLI1\n    EPW --&gt; CLI1\n\n    CLI1 --&gt; EXP\n    EXP --&gt; FC\n    EXP --&gt; GPC\n    EXP --&gt; HDC\n\n    FC --&gt; GISPROCESS\n    GPC --&gt; GISPROCESS\n    GIS --&gt; GP1\n    SF --&gt; GP4\n    EPW --&gt; GP16\n\n    GP16 --&gt; A1\n    A1 --&gt; A2\n    A2 --&gt; A3\n    A3 --&gt; A4\n    A4 --&gt; A5\n    A5 --&gt; H\n\n    W1 --&gt; S1\n    W2 --&gt; S1\n    WN --&gt; S1\n\n    CDB --&gt; S3\n    S8 --&gt; R1\n\n    R1 --&gt; R2\n    R2 --&gt; R3\n    R3 --&gt; R4\n    R4 --&gt; S3\n\n    CLI3 --&gt; O1\n    S3 --&gt; O1\n    O1 --&gt; O2\n    O1 --&gt; O3\n    O2 --&gt; O4\n    O2 --&gt; O5\n\n    CLI5 --&gt; O4\n\n    %% External dependency connections\n    S5 -.uses.-&gt; EPL\n    S3 -.uses.-&gt; EPI\n    S3 -.uses.-&gt; ARC\n    A5 -.uses.-&gt; SCY\n    S3 -.uses.-&gt; PRIS\n\n    %% Styling - using medium-toned colors for better contrast\n    style INPUTS fill:#60a5fa,stroke:#2563eb,stroke-width:3px,color:#000\n    style CLI fill:#d1d5db,stroke:#6b7280,stroke-width:3px,color:#000\n    style CONFIG fill:#fcd34d,stroke:#f59e0b,stroke-width:3px,color:#000\n    style GISPROCESS fill:#fcd34d,stroke:#f59e0b,stroke-width:3px,color:#000\n    style ALLOC fill:#fca5a5,stroke:#dc2626,stroke-width:3px,color:#000\n    style DIST fill:#fca5a5,stroke:#dc2626,stroke-width:3px,color:#000\n    style SIM fill:#fca5a5,stroke:#dc2626,stroke-width:3px,color:#000\n    style AGG fill:#4ade80,stroke:#16a34a,stroke-width:3px,color:#000\n    style OUTPUT fill:#4ade80,stroke:#16a34a,stroke-width:3px,color:#000\n    style EXT fill:#d8b4fe,stroke:#9333ea,stroke-width:3px,color:#000</code></pre>"},{"location":"architecture-detailed/#component-details","title":"Component Details","text":""},{"location":"architecture-detailed/#system-inputs","title":"System Inputs","text":""},{"location":"architecture-detailed/#manifest-file-globiexperimentspec","title":"Manifest File (GloBIExperimentSpec)","text":"<ul> <li>Experiment name and scenario identifier</li> <li>File paths configuration</li> <li>GIS preprocessor parameters (thresholds, defaults, CRS)</li> <li>Hourly data extraction settings (optional)</li> </ul>"},{"location":"architecture-detailed/#gis-building-data","title":"GIS Building Data","text":"<ul> <li>Building footprints as polygons (Shapefile/GeoJSON/GeoPackage)</li> <li>Properties: height, number of floors, typology, age, region</li> <li>Coordinate reference system (CRS) information</li> </ul>"},{"location":"architecture-detailed/#component-database","title":"Component Database","text":"<ul> <li>SQLite database accessed via Prisma ORM</li> <li>Building components: walls, windows, roofs, floors</li> <li>Material properties and thermal characteristics</li> <li>Accessed during simulation to construct energy models</li> </ul>"},{"location":"architecture-detailed/#semantic-fields-component-map","title":"Semantic Fields &amp; Component Map","text":"<ul> <li>YAML files defining categorical building attributes</li> <li>Maps building typologies to component selections</li> <li>Examples: residential/commercial, construction era, climate zone</li> </ul>"},{"location":"architecture-detailed/#weather-data","title":"Weather Data","text":"<ul> <li>EPW (EnergyPlus Weather) files or archive</li> <li>Can be queried dynamically based on building location</li> <li>Provides hourly climate data for simulation</li> </ul>"},{"location":"architecture-detailed/#cli-layer","title":"CLI Layer","text":"<p>The command-line interface provides user-facing commands:</p> <ul> <li><code>submit manifest</code>: Load experiment configuration and initiate preprocessing/allocation</li> <li><code>simulate</code>: Run single building simulation (testing/debugging)</li> <li><code>get experiment</code>: Retrieve results from cloud storage</li> <li><code>ui</code>: Launch Streamlit web interface for interactive exploration</li> <li><code>output_viz</code>: Generate D3 visualization dashboards from results</li> </ul>"},{"location":"architecture-detailed/#configuration-layer","title":"Configuration Layer","text":""},{"location":"architecture-detailed/#globiexperimentspec","title":"GloBIExperimentSpec","text":"<ul> <li>Top-level experiment configuration</li> <li>Links to FileConfig, GISPreprocessorConfig, HourlyDataConfig</li> <li>Supports manifest loading from YAML files</li> </ul>"},{"location":"architecture-detailed/#fileconfig","title":"FileConfig","text":"<ul> <li>Paths to all required input files</li> <li>File validation and existence checks</li> </ul>"},{"location":"architecture-detailed/#gispreprocessorconfig","title":"GISPreprocessorConfig","text":"<ul> <li>Geometric filtering thresholds (min/max area, edge length)</li> <li>Default values (height, WWR, basement, attic)</li> <li>CRS projection settings</li> <li>Weather query parameters</li> </ul>"},{"location":"architecture-detailed/#hourlydataconfig","title":"HourlyDataConfig","text":"<ul> <li>Variables to extract from EnergyPlus SQL output</li> <li>Enables optional hourly timeseries capture</li> </ul>"},{"location":"architecture-detailed/#gis-preprocessing-pipeline","title":"GIS Preprocessing Pipeline","text":"<p>The preprocessing pipeline transforms raw GIS data into simulation-ready building specifications:</p> <ol> <li>Load &amp; Validate: Read GIS file into GeoDataFrame, validate schema</li> <li>Reproject: Convert to Cartesian CRS for geometric operations</li> <li>Column Mapping: Handle Shapefile 10-character column name limits</li> <li>Semantic Validation: Ensure semantic fields exist in GIS data</li> <li>ID Handling: Generate UUIDs for buildings without IDs</li> <li>Coordinate Extraction: Extract latitude/longitude for weather queries</li> <li>Property Filters: Filter by height, floors, WWR, basement, attic</li> <li>Geometry Processing:</li> <li>Remove invalid geometries (non-polygons, self-intersections)</li> <li>Convert to rotated rectangles (<code>gis/geometry.py</code>)</li> <li>Filter by minimum/maximum building area</li> <li>Filter by minimum/maximum edge length</li> <li>Neighbor Analysis: Identify adjacent buildings for shading calculations</li> <li>Semantic Context: Inject building typology, age, region metadata</li> <li>Weather Assignment: Match buildings to EPW files by location</li> </ol> <p>Output: Clean GeoDataFrame with enriched building data and column mappings</p>"},{"location":"architecture-detailed/#allocation-layer","title":"Allocation Layer","text":"<p>Prepares building specifications for distributed execution:</p> <ol> <li>Spec Generation: Create <code>GloBIBuildingSpec</code> for each building row</li> <li>Extract geometry (rotated rectangle, neighbors)</li> <li>Extract properties (height, floors, WWR, basement, attic)</li> <li> <p>Link semantic context and weather file</p> </li> <li> <p>Branching Factor Calculation:</p> </li> <li>Sample 1000 random specs</li> <li>Measure average JSON payload size</li> <li>Calculate: <code>sims_per_branch = 3MB / avg_size</code></li> <li> <p>Determine: <code>branches_required = total_specs / sims_per_branch</code></p> </li> <li> <p>Job Submission:</p> </li> <li>Create <code>BaseExperiment</code> with <code>simulate_globi_building</code> function</li> <li>Configure <code>RecursionMap</code> for distribution strategy</li> <li>Submit to Hatchet with S3 client for result storage</li> </ol> <p>Output: Hatchet job reference and run metadata</p>"},{"location":"architecture-detailed/#distributed-computing-infrastructure","title":"Distributed Computing Infrastructure","text":""},{"location":"architecture-detailed/#hatchet-workflow-orchestrator","title":"Hatchet Workflow Orchestrator","text":"<ul> <li>Receives job submissions from allocation layer</li> <li>Manages task queue and worker assignment</li> <li>Handles retries and error recovery</li> <li>Tracks job progress and completion status</li> </ul>"},{"location":"architecture-detailed/#docker-workers","title":"Docker Workers","text":"<ul> <li>Containerized execution environments</li> <li>Pre-configured with EnergyPlus, EPInterface, dependencies</li> <li>Scale horizontally based on workload</li> <li>Stream results to cloud storage via Scythe framework</li> </ul>"},{"location":"architecture-detailed/#s3cloud-storage","title":"S3/Cloud Storage","text":"<ul> <li>Stores simulation results as Parquet files</li> <li>Versions experiments using semantic versioning</li> <li>Provides durable storage for large-scale experiments</li> </ul>"},{"location":"architecture-detailed/#energy-simulation-pipeline","title":"Energy Simulation Pipeline","text":"<p>Each worker executes the following for assigned buildings:</p> <ol> <li>Receive Spec: Deserialize <code>GloBIBuildingSpec</code> from JSON</li> <li>Zone Definition: Construct building zones from semantic fields and component map</li> <li>Model Construction: Use EPInterface/Archetypal to build EnergyPlus IDF</li> <li>Validation: Check conditioned floor areas match geometry</li> <li>Simulation: Run EnergyPlus simulation via <code>model.run()</code></li> <li>Results Extraction:</li> <li>Query SQL output for monthly energy and peak results</li> <li>Create MultiIndex DataFrame (Measurement, Feature levels)</li> <li>Optionally extract hourly timeseries data</li> <li>Output Creation: Build <code>GloBIOutputSpec</code> with results and metadata</li> </ol> <p>Output: <code>GloBIOutputSpec</code> with DataFrames and hourly data references</p>"},{"location":"architecture-detailed/#results-aggregation","title":"Results Aggregation","text":"<p>The Scythe framework handles result consolidation:</p> <ol> <li>Collection: Gather <code>GloBIOutputSpec</code> objects from all workers</li> <li>Aggregation: Concatenate DataFrames across buildings</li> <li>Versioning: Apply semantic version to experiment results</li> <li>Storage: Write aggregated Parquet files to S3</li> </ol> <p>Output: Versioned experiment results in cloud storage</p>"},{"location":"architecture-detailed/#output-layer","title":"Output Layer","text":"<p>Results are delivered to users via:</p> <ol> <li>Download: Retrieve Parquet files from S3 to local directory</li> <li><code>Results.parquet</code>: Monthly energy and peak data</li> <li> <p><code>HourlyData.parquet</code>: Optional hourly timeseries</p> </li> <li> <p>Visualization: Generate interactive D3 dashboards</p> </li> <li>Summary statistics (mean, min, max)</li> <li>Energy use intensity (EUI) distributions</li> <li> <p>Peak demand analysis</p> </li> <li> <p>CSV Export: Convert Parquet to CSV for external analysis tools</p> </li> </ol>"},{"location":"architecture-detailed/#external-dependencies","title":"External Dependencies","text":""},{"location":"architecture-detailed/#energyplus","title":"EnergyPlus","text":"<p>Building energy simulation engine that performs physics-based thermal calculations</p>"},{"location":"architecture-detailed/#epinterface","title":"EPInterface","text":"<p>Python library for generating EnergyPlus IDF (Input Data File) models programmatically</p>"},{"location":"architecture-detailed/#archetypal","title":"Archetypal","text":"<p>Provides building archetype templates and simplified building energy modeling (SBEM)</p>"},{"location":"architecture-detailed/#scythe","title":"Scythe","text":"<p>Distributed computing framework for experiment allocation, result aggregation, and storage</p>"},{"location":"architecture-detailed/#prisma","title":"Prisma","text":"<p>Database ORM for accessing component database during model construction</p>"},{"location":"architecture-detailed/#key-design-principles","title":"Key Design Principles","text":"<ol> <li>Separation of Concerns: Clear boundaries between GIS processing, allocation, simulation, and results</li> <li>Scalability: Horizontal scaling via distributed workers and cloud storage</li> <li>Reproducibility: Version-controlled experiments with full provenance tracking</li> <li>Flexibility: Configurable preprocessing, semantic mappings, and simulation parameters</li> <li>Fault Tolerance: Retry logic and error handling throughout the pipeline</li> <li>Data Efficiency: Parquet format for compressed, columnar data storage</li> <li>Modularity: Independent components can be tested and deployed separately</li> </ol>"},{"location":"architecture-detailed/#data-flow-summary","title":"Data Flow Summary","text":"<pre><code>User Manifest\n    \u2193\nCLI loads configuration\n    \u2193\nGIS preprocessing enriches building data\n    \u2193\nAllocation creates building specs\n    \u2193\nHatchet distributes specs to workers\n    \u2193\nWorkers run EnergyPlus simulations\n    \u2193\nResults aggregated and stored in S3\n    \u2193\nCLI downloads and visualizes results\n    \u2193\nUser analyzes building stock performance\n</code></pre> <p>This architecture enables regional-scale building energy modeling with minimal manual intervention, supporting urban planning, policy analysis, and decarbonization strategies.</p>"},{"location":"architecture-simple/","title":"GloBI Architecture - Simple Overview","text":"<p>This diagram provides a high-level overview of the GloBI (Global Building Intelligence) system workflow.</p> <pre><code>%%{init: {'theme':'base', 'themeVariables': { 'primaryColor':'#4a9eff','primaryTextColor':'#000','primaryBorderColor':'#2563eb','lineColor':'#64748b','secondaryColor':'#fbbf24','tertiaryColor':'#34d399','noteBkgColor':'#fef3c7','noteTextColor':'#000','noteBorderColor':'#f59e0b'}}}%%\nflowchart TD\n    %% Input Stage\n    A[User Manifest File&lt;br/&gt;YAML Configuration] --&gt; B{CLI Command&lt;br/&gt;submit manifest}\n\n    %% Input Files\n    C[GIS Building Data&lt;br/&gt;Shapefile/GeoJSON] --&gt; D\n    E[Component Database&lt;br/&gt;SQLite/Prisma] --&gt; D\n    F[Semantic Fields&lt;br/&gt;YAML Mappings] --&gt; D\n    G[Weather Data&lt;br/&gt;EPW Files] --&gt; D\n\n    %% Preprocessing Stage\n    B --&gt; D[1. GIS Preprocessing&lt;br/&gt;Validate &amp; Enrich Data]\n\n    %% Spec Generation\n    D --&gt; H[2. Building Spec Generation&lt;br/&gt;Create GloBIBuildingSpec]\n\n    %% Allocation Stage\n    H --&gt; I[3. Job Allocation&lt;br/&gt;Calculate Branching&lt;br/&gt;&amp; Submit to Hatchet]\n\n    %% Distributed Execution\n    I --&gt; J[4. Distributed Simulation&lt;br/&gt;Workers Run EnergyPlus]\n\n    %% Results Collection\n    J --&gt; K[5. Results Aggregation&lt;br/&gt;Store in S3/Cloud]\n\n    %% Output Stage\n    K --&gt; L{CLI Command&lt;br/&gt;get experiment}\n    L --&gt; M[Local Results&lt;br/&gt;Parquet/CSV Files]\n    L --&gt; N[Visualization Dashboard&lt;br/&gt;Interactive HTML]\n\n    %% Styling\n    style A fill:#60a5fa,stroke:#2563eb,stroke-width:2px,color:#000\n    style C fill:#60a5fa,stroke:#2563eb,stroke-width:2px,color:#000\n    style E fill:#60a5fa,stroke:#2563eb,stroke-width:2px,color:#000\n    style F fill:#60a5fa,stroke:#2563eb,stroke-width:2px,color:#000\n    style G fill:#60a5fa,stroke:#2563eb,stroke-width:2px,color:#000\n    style D fill:#fcd34d,stroke:#f59e0b,stroke-width:2px,color:#000\n    style H fill:#fcd34d,stroke:#f59e0b,stroke-width:2px,color:#000\n    style I fill:#fca5a5,stroke:#dc2626,stroke-width:2px,color:#000\n    style J fill:#fca5a5,stroke:#dc2626,stroke-width:2px,color:#000\n    style K fill:#4ade80,stroke:#16a34a,stroke-width:2px,color:#000\n    style M fill:#4ade80,stroke:#16a34a,stroke-width:2px,color:#000\n    style N fill:#4ade80,stroke:#16a34a,stroke-width:2px,color:#000</code></pre>"},{"location":"architecture-simple/#workflow-stages","title":"Workflow Stages","text":""},{"location":"architecture-simple/#1-input-configuration","title":"1. Input Configuration","text":"<ul> <li>Manifest File: YAML configuration defining the experiment, file paths, and preprocessing parameters</li> <li>GIS Data: Building footprints with properties (height, floors, typology)</li> <li>Component Database: Building components and materials specifications</li> <li>Semantic Fields: Mappings between building categories and properties</li> <li>Weather Data: EPW climate files for simulation</li> </ul>"},{"location":"architecture-simple/#2-gis-preprocessing","title":"2. GIS Preprocessing","text":"<p>Validates and enriches building data: - Filters buildings by area, height, and geometry validity - Converts polygons to rotated rectangles - Identifies neighboring buildings for shading analysis - Assigns weather files based on location - Injects semantic context (building typology, age, region)</p>"},{"location":"architecture-simple/#3-building-spec-generation","title":"3. Building Spec Generation","text":"<p>Creates structured specifications for each building: - Extracts geometry (footprint dimensions, height, neighbors) - Assigns building properties (WWR, basement, attic) - Links semantic context and weather data - Produces <code>GloBIBuildingSpec</code> objects ready for simulation</p>"},{"location":"architecture-simple/#4-job-allocation","title":"4. Job Allocation","text":"<p>Distributes work across compute infrastructure: - Calculates optimal branching factor based on payload size - Submits jobs to Hatchet workflow orchestrator - Distributes building specs across Docker worker containers</p>"},{"location":"architecture-simple/#5-distributed-simulation","title":"5. Distributed Simulation","text":"<p>Workers execute energy simulations in parallel: - Each worker processes assigned building specs - Uses component database to construct EnergyPlus models - Runs building energy simulations - Extracts monthly energy and peak results - Optionally captures hourly timeseries data</p>"},{"location":"architecture-simple/#6-results-aggregation-collection","title":"6. Results Aggregation &amp; Collection","text":"<p>Consolidates simulation outputs: - Aggregates results from all workers - Stores in cloud storage (S3) as Parquet files - Versions experiments for reproducibility - Makes results available for retrieval</p>"},{"location":"architecture-simple/#7-output-visualization","title":"7. Output &amp; Visualization","text":"<p>Delivers results to users: - Downloads results to local filesystem - Generates interactive D3-based dashboards - Provides both Parquet and CSV formats - Enables analysis of building stock energy performance</p>"},{"location":"architecture-simple/#key-features","title":"Key Features","text":"<ul> <li>Scalability: Processes thousands of buildings in parallel using distributed computing</li> <li>Automation: Minimal manual intervention from GIS data to simulation results</li> <li>Reproducibility: Version-controlled experiments with semantic versioning</li> <li>Flexibility: Configurable preprocessing, semantic mappings, and output options</li> <li>Regional Analysis: Designed for urban-scale building energy modeling</li> </ul>"},{"location":"repo/","title":"Repo","text":""},{"location":"repo/#setup-steps","title":"Setup Steps","text":""},{"location":"repo/#step-1-clone-the-repository","title":"Step 1: Clone the Repository","text":"<pre><code>git clone {ADD THE REPO NAME HERE}\ncd globi\n</code></pre>"},{"location":"repo/#todo-remove-this-step","title":"TODO: remove this step","text":""},{"location":"repo/#step-3-install-project-dependencies","title":"Step 3: Install Project Dependencies","text":"<p>Install all project dependencies using <code>uv</code> and <code>make</code>:</p> <pre><code>make install\n</code></pre> <p>This will install all required Python packages defined in <code>pyproject.toml</code>.</p>"},{"location":"reference/cli/","title":"CLI","text":"<p>A cli is provided.</p>"},{"location":"reference/cli/#globi","title":"globi","text":"<p>The GloBI CLI.</p> <p>Use this to create, manage, and submit GloBI experiments.</p> <p>Usage:</p> <pre><code>globi [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>get: Get a GloBI experiment from different sources.</li> <li>output-viz: Generate a simple d3 based dashboard from globi outputs.</li> <li>simulate: Simulate a GloBI building.</li> <li>submit: Submit a GloBI experiment from different sources.</li> <li>ui: Launch the GloBI Streamlit UI for interactive configuration and simulation.</li> </ul>"},{"location":"reference/cli/#get","title":"get","text":"<p>Get a GloBI experiment from different sources.</p> <p>Usage:</p> <pre><code>globi get [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>experiment: Get a GloBI experiment from a manifest file.</li> </ul>"},{"location":"reference/cli/#experiment","title":"experiment","text":"<p>Get a GloBI experiment from a manifest file.</p> <p>Usage:</p> <pre><code>globi get experiment [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --run-name TEXT         The name of the run to get.  [required]\n  --version TEXT          The version of the run to get.\n  --dataframe-key TEXT    The dataframe to get.\n  --output-dir DIRECTORY  The path to the directory to use for the simulation.\n  --include-csv           Include the csv file in the output.\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#output-viz","title":"output-viz","text":"<p>Generate a simple d3 based dashboard from globi outputs.</p> <p>Usage:</p> <pre><code>globi output-viz [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --data-dir DIRECTORY  directory containing parquet output files  [default:\n                        tests/data/output]\n  --html-path FILE      path for the generated html dashboard (default: &lt;data-\n                        dir&gt;/output_viz.html)\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#simulate","title":"simulate","text":"<p>Simulate a GloBI building.</p> <p>Usage:</p> <pre><code>globi simulate [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --config FILE           The path to the minimal building spec file which\n                          will be used to configure the building.\n  --output-dir DIRECTORY  The path to the directory to use for the simulation.\n  --help                  Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#submit","title":"submit","text":"<p>Submit a GloBI experiment from different sources.</p> <p>Usage:</p> <pre><code>globi submit [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>manifest: Submit a GloBI experiment from a manifest file.</li> </ul>"},{"location":"reference/cli/#manifest","title":"manifest","text":"<p>Submit a GloBI experiment from a manifest file.</p> <p>Usage:</p> <pre><code>globi submit manifest [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --path PATH                     The path to the manifest file which will be\n                                  used to configure the experiment.\n  --scenario TEXT                 Override the scenario listed in the manifest\n                                  file with the provided scenario.\n  --skip-model-constructability-check\n                                  Skip the model constructability check.\n  --grid-run                      Dry run the experiment allocation by only\n                                  simulating semantic field combinations.\n  --epwzip-file PATH              Override the EPWZip file listed in the\n                                  manifest file with the provided EPWZip file.\n  --max-tests INTEGER             Override the maximum number of tests in a\n                                  grid run.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"reference/cli/#ui","title":"ui","text":"<p>Launch the GloBI Streamlit UI for interactive configuration and simulation.</p> <p>Usage:</p> <pre><code>globi ui [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --port INTEGER  Port to run the Streamlit UI on.\n  --host TEXT     Host to run the Streamlit UI on.\n  --help          Show this message and exit.\n</code></pre>"},{"location":"tutorials/getting-started/requirements/","title":"Setup Guide","text":"<p>This guide will walk you through downloading all of the required dependencies to be able to work with the <code>globi</code> repo.</p>"},{"location":"tutorials/getting-started/requirements/#what-youll-install","title":"What You'll Install","text":"<p>This repo requires five essential tools:</p> <ol> <li>Docker - Runs the Hatchet server locally in containers</li> <li>Git - Version control for managing code</li> <li>uv - Fast Python package manager</li> <li>Python 3.12+ - The programming language for this project</li> <li>make - Build automation/macro tool (Windows only; macOS includes this by default)</li> </ol>"},{"location":"tutorials/getting-started/requirements/#step-1-install-docker","title":"Step 1: Install Docker","text":"<p>Docker lets you run the Hatchet server in an isolated container so you don't have to worry about installing large applications locally. You'll need to download Docker Desktop.</p> macOSWindowsLinux <ol> <li>Download Docker Desktop for Mac</li> <li>Open the <code>.dmg</code> file and drag Docker to your Applications folder</li> <li>Launch Docker Desktop from Applications</li> <li>Verify the installation by opening Terminal and running:    <pre><code>docker --version\n</code></pre>    You should see output like: <code>Docker version 24.0.x</code></li> </ol> <p>Troubleshooting: Command not found</p> <p>If you see <code>docker: command not found</code>, Docker may not be in your PATH. Add it by running: <pre><code>export PATH=$PATH:$HOME/.docker/bin\n</code></pre> To make this permanent, add the line above to your <code>~/.zshrc</code> or <code>~/.bash_profile</code> file.</p> <ol> <li>Download Docker Desktop for Windows</li> <li>Run the installer and follow the installation wizard</li> <li>Restart your computer when prompted</li> <li>Launch Docker Desktop from the Start menu</li> <li>Verify the installation by opening PowerShell or Command Prompt and running:    <pre><code>docker --version\n</code></pre>    You should see output like: <code>Docker version 24.0.x</code></li> </ol> <ol> <li>Update your package index:    <pre><code>sudo apt-get update\n</code></pre></li> <li>Install Docker:    <pre><code>sudo apt-get install docker.io\n</code></pre></li> <li>Start Docker and enable it to start on boot:    <pre><code>sudo systemctl start docker\nsudo systemctl enable docker\n</code></pre></li> <li>Add your user to the docker group so you can run Docker without <code>sudo</code>:    <pre><code>sudo usermod -aG docker $USER\n</code></pre></li> <li>Log out and log back in for the group change to take effect</li> <li>Verify the installation:    <pre><code>docker --version\n</code></pre>    You should see output like: <code>Docker version 24.0.x</code></li> </ol>"},{"location":"tutorials/getting-started/requirements/#step-2-install-git","title":"Step 2: Install Git","text":"<p>Git is essential for version control. We will use this to clone the globi repo, so you need to have an account beforehand.</p>"},{"location":"tutorials/getting-started/requirements/#check-for-existing-installation","title":"Check for Existing Installation","text":"<p>Open your terminal and run:</p> <pre><code>git --version\n</code></pre> <p>If you see a version number like <code>git version 2.x.x</code>, you're all set! Skip to Step 3.</p>"},{"location":"tutorials/getting-started/requirements/#install-git","title":"Install Git","text":"<p>If Git isn't installed, visit the official Git installation page and follow the instructions for your operating system.</p> <p>After installation, verify it worked:</p> <pre><code>git --version\n</code></pre>"},{"location":"tutorials/getting-started/requirements/#step-3-install-uv-and-python","title":"Step 3: Install uv and Python","text":"<p>This project uses Python 3.12+ and <code>uv</code> for package management. We recommend installing <code>uv</code> first, then using it to manage Python versions.</p> macOSLinuxWindows <ol> <li>Install uv with a single command:</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <ol> <li>Verify the installation:</li> </ol> <pre><code>uv --version\n</code></pre> <p>You should see output like: <code>uv 0.x.x</code></p> <ol> <li>Check if you have Python 3.12+:</li> </ol> <pre><code>python --version\n</code></pre> <p>or</p> <pre><code>python3 --version\n</code></pre> <ol> <li>Install Python using uv if needed:</li> </ol> <p>If your Python version is below 3.12 or you don't have Python installed, use <code>uv</code> to install it:</p> <pre><code>uv python install 3.12\n</code></pre> <p>For more details, check the uv Python installation guide.</p> <ol> <li>Verify Python installation:    <pre><code>python --version\n</code></pre>    You should see: <code>Python 3.12.x</code> or higher</li> </ol> <ol> <li>Install uv with a single command:</li> </ol> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <ol> <li>Verify the installation:</li> </ol> <pre><code>uv --version\n</code></pre> <p>You should see output like: <code>uv 0.x.x</code></p> <ol> <li>Check if you have Python 3.12+:</li> </ol> <pre><code>python --version\n</code></pre> <p>or</p> <pre><code>python3 --version\n</code></pre> <ol> <li>Install Python using uv if needed:</li> </ol> <p>If your Python version is below 3.12 or you don't have Python installed, use <code>uv</code> to install it:</p> <pre><code>uv python install 3.12\n</code></pre> <p>For more details, check the uv Python installation guide.</p> <ol> <li>Verify Python installation:    <pre><code>python --version\n</code></pre>    You should see: <code>Python 3.12.x</code> or higher</li> </ol> <ol> <li>Install uv using PowerShell:</li> </ol> <pre><code>powershell -ExecutionPolicy ByPass -c \"irm https://astral.sh/uv/install.ps1 | iex\"\n</code></pre> <p>About ExecutionPolicy ByPass</p> <p>The <code>-ExecutionPolicy ByPass</code> flag temporarily allows running the installation script from the internet. This only applies to this single command and doesn't change your system settings.</p> <p>If you'd like to inspect the script before running it, you can view it first: <pre><code>powershell -c \"irm https://astral.sh/uv/install.ps1 | more\"\n</code></pre></p> <p>Alternatively, you can download the installer directly from GitHub.</p> <ol> <li>Verify the installation:</li> </ol> <pre><code>uv --version\n</code></pre> <p>You should see output like: <code>uv 0.x.x</code></p> <ol> <li>Check if you have Python 3.12+:</li> </ol> <pre><code>python --version\n</code></pre> <ol> <li>Install Python using uv if needed:</li> </ol> <p>If your Python version is below 3.12 or you don't have Python installed:</p> <pre><code>uv python install 3.12\n</code></pre> <ol> <li>Verify Python installation:    <pre><code>python --version\n</code></pre>    You should see: <code>Python 3.12.x</code> or higher</li> </ol>"},{"location":"tutorials/getting-started/requirements/#step-4-install-make","title":"Step 4: Install make","text":"macOSLinuxWindows <p>You already have <code>make</code> installed by default! You can verify by running:</p> <pre><code>make --version\n</code></pre> <p>You should see output like: <code>GNU Make 3.x</code> or <code>4.x</code></p> <p>You already have <code>make</code> installed by default! You can verify by running:</p> <pre><code>make --version\n</code></pre> <p>You should see output like: <code>GNU Make 4.x</code></p> <p>You'll need to install <code>make</code> to use the project's build commands.</p> <p>Option 1: Using winget (Recommended)</p> <p>Windows 10+ includes winget by default, so no separate package manager installation needed:</p> <pre><code>winget install GnuWin32.Make\n</code></pre> <p>After installation, you may need to restart your terminal or add <code>C:\\Program Files (x86)\\GnuWin32\\bin</code> to your PATH.</p> <p>Option 2: Using Git Bash</p> <p>If you installed Git for Windows in Step 2, you can use Git Bash terminal which includes <code>make</code>. Just open \"Git Bash\" instead of PowerShell or Command Prompt.</p> <p>Option 3: Using Chocolatey</p> <p>If you have Chocolatey installed:</p> <pre><code>choco install make\n</code></pre> <p>Option 4: Using Scoop</p> <p>If you have Scoop installed:</p> <pre><code>scoop install make\n</code></pre> <p>Verify Installation</p> <pre><code>make --version\n</code></pre> <p>You should see output like: <code>GNU Make 3.x</code> or <code>4.x</code></p>"},{"location":"tutorials/run-simulations/simulation_tasking/","title":"Simulation Tasking","text":""},{"location":"tutorials/run-simulations/simulation_tasking/#run-simulations-with-hatchet-and-docker","title":"Run simulations with Hatchet and Docker","text":"<p>This guide walks you through running <code>globi</code> simulations end\u2011to\u2011end using Hatchet and Docker.</p> <p>It assumes you have already completed the setup guide, including:</p> <ul> <li>cloning the <code>globi</code> repo</li> <li>installing dependencies with <code>uv sync --all-extras --all-groups</code></li> <li>installing Docker, Git, Python 3.12+, and <code>make</code></li> </ul> <p>The steps below cover:</p> <ul> <li>starting the Hatchet server and simulation engine</li> <li>configuring environment files and tokens</li> <li>submitting a simulation manifest</li> <li>monitoring runs in the Hatchet UI</li> <li>fetching and storing results</li> <li>safely shutting everything down</li> </ul>"},{"location":"tutorials/run-simulations/simulation_tasking/#before-you-start","title":"Before you start","text":"<ul> <li>docker running: make sure Docker Desktop (or the Docker daemon) is running.</li> <li>terminal location: run commands from the repository root (the folder containing <code>Makefile</code> and <code>pyproject.toml</code>).</li> <li>network access: the first run may download container images from remote registries and can take several minutes.</li> </ul> <p>Note</p> <p>the commands in this guide are the same for macOS, linux, and windows (using a unix\u2011like shell such as git bash or wsl).</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#step-1-start-the-hatchet-server","title":"Step 1: Start the Hatchet server","text":"<p>The Hatchet server provides the UI and orchestration backend for managing workflows.</p> first runsubsequent runs <p>Run:</p> <pre><code>make hatchet-lite\n</code></pre> <p>This:</p> <ul> <li>builds and/or pulls the <code>hatchet-lite</code> docker image</li> <li>starts the Hatchet server container in the background</li> <li>exposes the Hatchet UI on <code>http://localhost:8080</code></li> </ul> <p>Note</p> <p>the first run may take several minutes while docker downloads and builds images. later runs are much faster.</p> <p>If you have already started Hatchet once before, you can simply ensure it is running with:</p> <pre><code>make hatchet-lite\n</code></pre> <p>Docker will reuse existing images and start the container if it is not already running.</p> <p>You can verify the container is up by running:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.hatchet.yml ps\n</code></pre> <p>Look for a <code>hatchet-lite</code> container with a <code>running</code> status.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#step-2-create-and-configure-hatchet-environment-files","title":"Step 2: Create and configure Hatchet environment files","text":"<p>Hatchet uses a client token stored in environment files that are loaded by the <code>make cli</code> target.</p> <ol> <li>Generate a Hatchet client token:</li> </ol> <pre><code>make hatchet-token\n</code></pre> <p>This will:</p> <ul> <li>ensure <code>hatchet-lite</code> is running</li> <li>execute the Hatchet admin command inside the container</li> <li> <p>print a <code>HATCHET_CLIENT_TOKEN</code> value in your terminal</p> </li> <li> <p>Copy the token into your Hatchet env files.</p> </li> </ul> <p>In your terminal output, locate a line similar to:</p> <pre><code>HATCHET_CLIENT_TOKEN=your_generated_token_here\n</code></pre> <p>Open your Hatchet environment file(s), for example:</p> <ul> <li><code>.env.local.host.hatchet</code></li> </ul> <p>and add or update the line:</p> <pre><code>HATCHET_CLIENT_TOKEN=your_generated_token_here\n</code></pre> <ol> <li>Save the files.</li> </ol> <p>Warning</p> <p>treat your <code>HATCHET_CLIENT_TOKEN</code> like a password. do not commit it to git, and do not share it publicly.</p> <p>Tip</p> <p>if you see example files such as <code>.env.local.host.hatchet.example</code>, copy them once and then edit the resulting <code>.env</code> files:</p> <pre><code>cp .env.local.host.hatchet.example .env.local.host.hatchet\n</code></pre> <p>then replace the placeholder token with the real one.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#step-3-start-the-simulation-engine-and-workers","title":"Step 3: Start the simulation engine and workers","text":"<p>Now start the full engine stack, which includes:</p> <ul> <li>Hatchet server</li> <li>simulation workers</li> <li>fanout workers</li> <li>any required supporting services</li> </ul> <p>Run:</p> <pre><code>make engine\n</code></pre> <p>This command:</p> <ul> <li>composes <code>docker-compose.yml</code>, <code>docker-compose.hatchet.yml</code>, and <code>docker-compose.aws.yml</code></li> <li>builds images if needed</li> <li>starts all services in the background with <code>-d</code></li> </ul> <p>You can check container status with:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.hatchet.yml -f docker-compose.aws.yml ps\n</code></pre> <p>You should see containers for Hatchet and the simulation services with a <code>running</code> status.</p> <p>Note</p> <p>on macos, you may occasionally see an error like:</p> <pre><code>target simulations: failed to solve: image \".../hatchet/globi:latest\": already exists\n\nmake: *** [engine] Error 1\n</code></pre> <p>this is usually transient. re\u2011run:</p> <pre><code>make engine\n</code></pre> <p>and the issue should resolve.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#step-4-access-the-hatchet-ui","title":"Step 4: Access the Hatchet UI","text":"<p>Open your browser and go to:</p> <pre><code>http://localhost:8080\n</code></pre> <p>On the first run, Hatchet may prompt you to create or confirm an admin account in the terminal where the container is running.</p> <p>For the local <code>hatchet-lite</code> instance, you can use:</p> <pre><code>username: admin@example.com\npassword: Admin123!!\n</code></pre> <p>In the Hatchet UI:</p> <ul> <li>navigate to workers</li> <li>verify that the expected workers are running and healthy</li> </ul> <p>If you do not see workers, refer to the troubleshooting section below.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#step-5-run-a-test-simulation","title":"Step 5: Run a test simulation","text":"<p>Now you can submit a simulation manifest via the <code>make cli</code> target, which wraps the <code>globi</code> CLI with the correct environment files.</p> <ol> <li> <p>Confirm the engine is running:</p> </li> <li> <p>ensure <code>make engine</code> has completed without errors</p> </li> <li> <p>verify containers are running with <code>docker compose ... ps</code></p> </li> <li> <p>Decide which manifest to run.</p> </li> </ol> <p>A typical manifest path looks like:</p> <pre><code>data/partners/LOCATION/manifest.yml\n</code></pre> <p>Replace <code>LOCATION</code> with the specific dataset or project you want to simulate.</p> <ol> <li>Submit the manifest:</li> </ol> <pre><code>make cli submit manifest -- --path {PATH_TO_MANIFEST} --grid-run\n</code></pre> <p>where:</p> <ul> <li><code>{PATH_TO_MANIFEST}</code> is your manifest file path (for example <code>data/partners/Cambridge_UK/manifest.yml</code>)</li> <li> <p><code>--grid-run</code> enables grid\u2011style execution over the manifest configuration</p> </li> <li> <p>Monitor progress in the Hatchet UI:</p> </li> <li> <p>go to <code>http://localhost:8080</code></p> </li> <li>navigate to workflows or runs</li> <li>locate the workflow corresponding to your manifest submission</li> <li>watch status transition from <code>pending</code> \u2192 <code>running</code> \u2192 <code>completed</code> (or <code>failed</code> if there is an error)</li> </ul> <p>You can click into the workflow to view task\u2011level logs and any errors.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#step-6-fetch-simulation-results","title":"Step 6: Fetch simulation results","text":"<p>When a simulation completes, the CLI prints a summary similar to:</p> <pre><code>versioned_experiment:\n  base_experiment:\n    experiment: scythe_experiment_simulate_globi_building\n    run_name: Cambridge_UK/dryrun/Baseline\n    storage_settings:\n      BUCKET: globi-bucket\n      BUCKET_PREFIX: globi\n  version:\n    major: 1\n    minor: 0\n    patch: 0\ntimestamp: '2026-01-23T13:57:36.233154'\n</code></pre> <ul> <li>run_name identifies the specific run (for example <code>Cambridge_UK/dryrun/Baseline</code>)</li> <li>version is a semantic version (major.minor.patch) of the experiment configuration</li> </ul>"},{"location":"tutorials/run-simulations/simulation_tasking/#fetch-the-latest-version-of-a-run","title":"Fetch the latest version of a run","text":"<p>Copy the <code>run_name</code> from the output and run:</p> <pre><code>make cli get experiment --run-name {YOUR_RUN_NAME_HERE}\n</code></pre> <p>This fetches the latest available version of that experiment and stores the results in your configured storage/output location. The CLI prints where the results were saved.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#fetch-a-specific-version-and-output-directory","title":"Fetch a specific version and output directory","text":"<p>If you have multiple versions of the same run, or you want to control exactly where results are written, include <code>--version</code> and <code>--output_dir</code>:</p> <pre><code>make cli get experiment \\\n  --run-name {YOUR_RUN_NAME_HERE} \\\n  --version {VERSION} \\\n  --output_dir {YOUR_CHOSEN_OUTPUT_DIR}\n</code></pre> <p>where:</p> <ul> <li><code>{VERSION}</code> is of the form <code>major.minor.patch</code> (for example <code>1.0.0</code>)</li> <li><code>{YOUR_CHOSEN_OUTPUT_DIR}</code> is a local path where you want results saved</li> </ul> <p>Tip</p> <p>choose an output directory under a dedicated folder (for example <code>outputs/</code>) to keep simulation results organized by run and version.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#step-7-shut-down-docker-services","title":"Step 7: Shut down Docker services","text":"<p>When you are done running simulations, you can stop all related Docker containers with:</p> <pre><code>make down\n</code></pre> <p>This:</p> <ul> <li>stops and removes containers from <code>docker-compose.yml</code>, <code>docker-compose.hatchet.yml</code>, and <code>docker-compose.aws.yml</code></li> <li>keeps docker images on disk so future runs start faster</li> </ul> <p>Run <code>make engine</code> again the next time you want to use the system.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#troubleshooting","title":"Troubleshooting","text":"<p>This section lists common issues and concrete steps to diagnose and fix them.</p>"},{"location":"tutorials/run-simulations/simulation_tasking/#docker-and-container-issues","title":"Docker and container issues","text":"<ul> <li> <p>docker daemon not running</p> </li> <li> <p>ensure Docker Desktop (macOS/windows) or the docker service (linux) is running</p> </li> <li> <p>verify with:</p> <pre><code>docker --version\ndocker ps\n</code></pre> </li> <li> <p>containers not staying up</p> </li> <li> <p>check logs for a specific service, for example:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.hatchet.yml -f docker-compose.aws.yml logs hatchet-lite\n</code></pre> </li> <li> <p>look for configuration or startup errors in the log output</p> </li> <li> <p>image already exists error when running <code>make engine</code></p> </li> <li> <p>if you see:</p> <pre><code>target simulations: failed to solve: image \".../hatchet/globi:latest\": already exists\n</code></pre> </li> <li> <p>simply re\u2011run:</p> <pre><code>make engine\n</code></pre> </li> <li> <p>if the error persists, run:</p> <pre><code>make down\nmake engine\n</code></pre> </li> <li> <p>port 8080 already in use</p> </li> <li> <p>if <code>hatchet-lite</code> fails to start because port <code>8080</code> is in use:</p> <ul> <li>close any other application using port <code>8080</code></li> <li>or stop the conflicting container/process</li> <li>then re\u2011run <code>make hatchet-lite</code> or <code>make engine</code></li> </ul> </li> </ul>"},{"location":"tutorials/run-simulations/simulation_tasking/#hatchet-ui-and-api-issues","title":"Hatchet UI and API issues","text":"<ul> <li> <p>cannot load <code>http://localhost:8080</code></p> </li> <li> <p>verify that <code>hatchet-lite</code> is running:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.hatchet.yml ps\n</code></pre> </li> <li> <p>if the container is not <code>running</code>, start it:</p> <pre><code>make hatchet-lite\n</code></pre> </li> <li> <p>if it still fails, inspect logs:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.hatchet.yml logs hatchet-lite\n</code></pre> </li> <li> <p>workers not appearing in Hatchet UI</p> </li> <li> <p>ensure the engine stack is running:</p> <pre><code>make engine\n</code></pre> </li> <li> <p>check for worker containers in <code>docker compose ... ps</code></p> </li> <li>open Hatchet UI \u2192 workers and verify that they show as healthy</li> <li>if workers crash repeatedly, inspect their logs using <code>docker compose ... logs &lt;service-name&gt;</code></li> </ul>"},{"location":"tutorials/run-simulations/simulation_tasking/#token-and-environment-configuration-issues","title":"Token and environment configuration issues","text":"<ul> <li> <p>token errors or unauthorized requests</p> </li> <li> <p>confirm <code>HATCHET_CLIENT_TOKEN</code> is set in your Hatchet env file(s), for example:</p> <pre><code>HATCHET_CLIENT_TOKEN=your_generated_token_here\n</code></pre> </li> <li> <p>ensure there are no extra quotes or spaces around the value</p> </li> <li> <p>if you suspect the token is invalid or expired:</p> <pre><code>make hatchet-token\n</code></pre> <p>then update the env files with the new token.</p> </li> <li> <p>env file not being loaded</p> </li> <li> <p><code>make cli</code> loads environment from:</p> <ul> <li><code>.env.$(AWS_ENV).aws</code> (default: <code>.env.local.host.aws</code>)</li> <li><code>.env.$(HATCHET_ENV).hatchet</code> (default: <code>.env.local.host.hatchet</code>)</li> <li><code>.env.scythe.fanouts</code></li> <li><code>.env.scythe.storage</code></li> </ul> </li> <li> <p>verify these files exist and contain the expected variables</p> </li> </ul>"},{"location":"tutorials/run-simulations/simulation_tasking/#simulation-and-worker-issues","title":"Simulation and worker issues","text":"<ul> <li> <p>jobs stuck in <code>pending</code></p> </li> <li> <p>check that workers are running (Hatchet UI \u2192 workers)</p> </li> <li>confirm worker containers are healthy with <code>docker compose ... ps</code></li> <li> <p>inspect worker logs for errors (for example configuration or connectivity issues)</p> </li> <li> <p>workflow fails immediately after submission</p> </li> <li> <p>open the workflow in the Hatchet UI and inspect task logs</p> </li> <li>common causes:<ul> <li>invalid manifest path (<code>--path</code> does not exist)</li> <li>missing or incorrect environment variables</li> <li>storage configuration issues (for example s3 bucket permissions)</li> </ul> </li> </ul>"},{"location":"tutorials/run-simulations/simulation_tasking/#python-and-uv-issues","title":"Python and uv issues","text":"<ul> <li> <p><code>module not found</code> or missing dependency</p> </li> <li> <p>re\u2011sync dependencies:</p> <pre><code>uv sync --all-extras --all-groups\n</code></pre> </li> <li> <p>or run the project install target:</p> <pre><code>make install\n</code></pre> </li> <li> <p>python version error</p> </li> <li> <p>confirm that your python version is 3.12 or higher:</p> <pre><code>python --version\npython3 --version\n</code></pre> </li> <li> <p>if needed, install python 3.12 with <code>uv</code> (see the setup guide).</p> </li> </ul>"},{"location":"tutorials/run-simulations/simulation_tasking/#quick-reference","title":"Quick reference","text":""},{"location":"tutorials/run-simulations/simulation_tasking/#essential-commands","title":"Essential commands","text":"<pre><code># start hatchet server (ui and api)\nmake hatchet-lite\n\n# generate hatchet token and print to terminal\nmake hatchet-token\n\n# start full engine stack (hatchet + workers + services)\nmake engine\n\n# submit a simulation manifest\nmake cli submit manifest -- --path data/partners/LOCATION/manifest.yml --grid-run\n\n# fetch experiment results\nmake cli get experiment --run-name {YOUR_RUN_NAME_HERE}\n\n# stop and remove all related docker containers\nmake down\n\n# open hatchet ui\nopen http://localhost:8080  # macos\n# or manually paste http://localhost:8080 into your browser\n</code></pre>"},{"location":"tutorials/run-simulations/simulation_tasking/#key-file-locations","title":"Key file locations","text":"<ul> <li>environment config: <code>.env.*</code> files used by <code>make cli</code></li> <li>data directory: <code>data/</code></li> <li>hatchet configuration: <code>hatchet.yaml</code></li> <li>make targets: <code>Makefile</code></li> </ul>"}]}